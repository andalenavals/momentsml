# Configuration File for Tenbilac
# WARNING: ConfigParser configuration files are surprisingly weird for newbies:
# - Comment only on separate lines
# - Leave a field blank AND REMOVE THE ":" to specify "None"
# - But the latter only works if a string was expected, not for floats, ints, ...

[setup]

# Pick a name for your training run (runs with different settings can potentially be mixed later)
name:
description: One mult layer, optimized with BFGS at the same time as the sum layer.

# Path to a workdir where networks and results will be or are stored.
# MegaLUT uses its own workdir, taking precedence over this setting.
workdir: ./tenbilac-workdir

copyconfig: True

minimize: True


[norm]

takeover: True

# Which normer to use on the inputs, if any
oninputs: True
inputtype: sa1
#inputtype: -11

# Same on the targets. If the activation function of your output nodes is linear (i.e., "iden"),
# and if you don't mind large weights (regul !), you usually don't need a target normer.
ontargets: False
targettype: -11

[net]

# Number of committee members
nmembers: 8

# The type of the networks (Net, MultNet)
type: Net
# Structure of hidden layers 
nhs: [-4,4]

# For MultNets, specify here the intial weights for the additional nodes of the first layer.
# Set it to an empty list if you don't want the extra nodes.
#mwlist: [(0.7,), (1.3,), (1, 1), (1, -1), (1, 0, 0, 1), (1, 0, 0, -1)]
#mwlist: [(0.75,), (1.25,), (0.6,), (1.4,)]

mwlist: [(1, 1), (1, -1)]


# Names of the activation functions to use for different layers
actfctname: tanh
oactfctname: iden
multactfctname: iden

# Preset the network to transport the first of its inputs as output ?
startidentity: True
# Limit the number of nodes per layer which simply transport the outputs from the previous layer
# Set this to -1 if you don't want such a limit.
onlynidentity: -1

# Parameters controlling the noise added to the networks prior to training.
# Noise is only added to new blank networks, not if previously trained networks are reused.
addnoise: True
ininoisewscale: 0.1
ininoisebscale: 0.1
ininoisemultwscale: 0.3
ininoisemultbscale: 0.0


[train]

# Takeover (i.e., reuse) potential existing trainings or start the trainings from scratch ?
takeover: True

# The number of cores on which to run. Set it to 0 to run on all cores, and to 1 to avoid using multiprocessing.
ncpu: 8

# Name of the error function (mse, msb, ...).
errfctname: msb


# Regul is not yet fully implemented
useregul: False
#regulfctname
#regulweight

# Generic training parameters
valfrac: 0.2
shuffle: True
mbfrac: 0.3
mbloops: 50


# Save plots ?
autoplot: True
# Plot (and track) the average prediction errors for each case ? Could be massive !
trackbiases: False
# Write the full status and history to disk at each iteration (not needed unless the system is unstable) ?
saveeachit: False
# Extra debug-logging at each call ?
verbose: False


# Choice of training algorithm
algo: bfgs


[algo_bfgs]

maxiter: 100
gtol: 1e-8


# One of the following Sections is passed as kwargs to the selected algorithm.
# The fields are read through an eval(), therefore to pass a string use repr("hello").
[algo_multnetbfgs]

nepochs: 1
maxiter_sum: 50
maxiter_mult: 0
gtol: 1e-8



[predict]

# How to combine the committee results (mean, median)
combine: mean

