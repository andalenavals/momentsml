# Configuration File for Tenbilac
# WARNING: ConfigParser configuration files are surprisingly weird for newbies:
# - Comment only on separate lines
# - Leave a field blank AND REMOVE THE ":" to specify "None"
# - But the latter only works if a string was expected, not for floats, ints, ...

[setup]

# Pick a name for your training run (runs with different settings can potentially be mixed later)
name: mlnet

# Path to a workdir where networks and results will be or are stored.
# MegaLUT uses its own workdir, taking precedence over this setting.
workdir: ./mlnet-workdir

copyconfig: True

# Should a "minimized" version (small file size, containing almost only the network parameters) of the
# committee be created after each training (useful to send the committee to someone else for predictions)?
minimize: True

[norm]

# If normers already exist in the workdir, should we reuse them (instead of
# overwriting them with new ones based on the current training data) ?
takeover: True

oninputs: True
inputtype: sa1
ontargets: True
targettype: -11

[net]

# Number of committee members
nmembers: 5

# The type of the networks (Net, MultNet)
type: Net
# Structure of hidden layers 
nhs: [5,5]

# For MultNets, specify here the intial weights for the additional nodes of the first layer.
# Set it to an empty list if you don't want the extra nodes.
#mwlist: [(0.7,), (1.3,), (1, 1), (1, -1), (1, 0, 0, 1), (1, 0, 0, -1)]
#mwlist: [(0.75,), (1.25,), (0.6,), (1.4,)]

mwlist: [(1, 1), (1, -1)]


# Names of the activation functions to use for different layers
actfctname: tanh
oactfctname: iden
multactfctname: iden

# Preset the network to transport the first of its inputs as output ?
startidentity: True
# Limit the number of nodes per layer which simply transport the outputs from the previous layer
# Set this to -1 if you don't want such a limit.
onlynidentity: -1

# Parameters controlling the noise added to the networks prior to training.
# Noise is only added to new blank networks, not if previously trained networks are reused.
addnoise: True
ininoisewscale: 0.1
ininoisebscale: 0.1
ininoisemultwscale: 0.1
ininoisemultbscale: 0.1


[train]

# Takeover (i.e., reuse) potential existing trainings or start the trainings from scratch ?
takeover: True

# The number of cores on which to run. Set it to 0 to run on all cores, and to 1 to avoid using multiprocessing.
ncpu: 5

# Name of the error function (mse, msb, ...).
errfctname: msb


# Regul is not yet fully implemented
useregul: False
#regulfctname
#regulweight

# Generic training parameters
valfrac: 0.2
shuffle: True
mbfrac: 0.2
mbloops: 400


# Save plots ?
autoplot: True
# Plot (and track) the average prediction errors for each case ? Could be massive !
trackbiases: False
# Write the full status and history to disk at each iteration (not needed unless the system is unstable) ?
saveeachit: False
# Extra debug-logging at each call ?
verbose: False


# Choice of training algorithm
algo: bfgs


[algo_bfgs]

maxiter: 50
gtol: 1e-8


# One of the following Sections is passed as kwargs to the selected algorithm.
# The fields are read through an eval(), therefore to pass a string use repr("hello").
[algo_multnetbfgs]

nepochs: 1
maxiter_sum: 50
maxiter_mult: 0
gtol: 1e-8



[predict]

# How to combine the committee results (mean, median)
combine: mean

